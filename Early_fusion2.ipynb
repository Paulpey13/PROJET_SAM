{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pydub import AudioSegment\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, BertModel, BertTokenizer, CamembertModel, CamembertTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "# Local application imports\n",
    "import load_data\n",
    "from training_audio_model import *\n",
    "from training_text_model import *\n",
    "from utils import *\n",
    "\n",
    "# Set the device for GPU usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the transcription files\n",
    "transcr_path = 'paco-cheese/transcr'\n",
    "\n",
    "# Load and preprocess all the Inter-Pausal Units (IPUs) from the transcription files\n",
    "data = load_data.load_all_ipus(folder_path=transcr_path, load_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the target variable 'y' from the DataFrame\n",
    "y = create_y(df)\n",
    "\n",
    "# Get the indices of the positive samples in 'y'\n",
    "indices = [i for i, x in enumerate(y) if x == 1]\n",
    "\n",
    "# Print the number of positive samples and their indices\n",
    "print(len(indices))\n",
    "print(indices)\n",
    "\n",
    "# Print the speaker corresponding to each positive sample\n",
    "for i in indices:\n",
    "    print(df['speaker'][i])\n",
    "\n",
    "# Display the last 5 entries of the DataFrame\n",
    "df[len(indices)-5:]\n",
    "\n",
    "# Look for entries in the DataFrame where the speaker is 'LS'\n",
    "ls = df[df['speaker'] == 'LS']\n",
    "ls\n",
    "\n",
    "# Check if there are any entries in the DataFrame where the speaker is missing\n",
    "df[df['speaker'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c779c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the audio files\n",
    "audio_files_path = 'paco-cheese/audio/2_channels/'\n",
    "\n",
    "# Extract the audio features from the data\n",
    "audio_segments = extract_audio_segments(data, audio_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f76990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "print(len(audio_segments))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "# Extract in a list the MFCC features for all audio segments, and convert the list to a numpy array\n",
    "X_audio = np.array([extract_features(segment) for segment in audio_segments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "# Save the current DataFrame 'df'\n",
    "df_save = df\n",
    "\n",
    "# Save the current audio features array 'X_audio'\n",
    "X_audio_save = X_audio\n",
    "\n",
    "# Save the current target variable array 'y'\n",
    "y_save = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840163bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "# Set a limit for the number of samples to use for testing\n",
    "limit = 1000  #107603  #110544\n",
    "X_audio = X_audio_save[:limit]\n",
    "y = y_save[:limit]\n",
    "df = df_save[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc9299",
   "metadata": {},
   "source": [
    "# Nouveau modele, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51233769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the textual data and extract the features\n",
    "# Make sure the 'data' variable is loaded as in your previous script\n",
    "text_features = data['text_words'][:limit]\n",
    "\n",
    "# Replace NaN values with a placeholder\n",
    "text_features = text_features.fillna('[UNK]')\n",
    "\n",
    "# Use CamemBERT tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Define a function to tokenize and pad the text to a maximum length of 256\n",
    "def tokenize_and_pad(text, max_len=256):\n",
    "    return tokenizer.encode(text, max_length=max_len, padding='max_length', truncation=True)\n",
    "\n",
    "# Apply the tokenizer to all textual data\n",
    "text_features = text_features.apply(lambda x: tokenize_and_pad(str(x)))\n",
    "\n",
    "# Make sure 'X' is your extracted audio features matrix\n",
    "audio_features = torch.tensor(X_audio)\n",
    "\n",
    "# Convert the textual features into a tensor\n",
    "text_features = torch.tensor(np.array(text_features.tolist()))\n",
    "\n",
    "# Merge the audio and textual features\n",
    "combined_features = torch.cat((audio_features, text_features), dim=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc94122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the class distribution in the training and test sets\n",
    "\n",
    "# Count the number of each class for y_train and y_test\n",
    "train_class_distribution = pd.Series(y_train).value_counts()\n",
    "test_class_distribution = pd.Series(y_test).value_counts()\n",
    "\n",
    "# Display\n",
    "print(\"Distribution of classes in the training set:\")\n",
    "print(train_class_distribution)\n",
    "print(\"Distribution of classes in the test set:\")\n",
    "print(test_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bf96a",
   "metadata": {},
   "outpsuts": [],
   "source": [
    "# Define the Early Fusion Model\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, audio_feature_size, text_feature_size, hidden_size=64, dropout_rate=0.5):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        # CamemBERT for text features\n",
    "        self.camembert = CamembertModel.from_pretrained('camembert-base')\n",
    "\n",
    "        # RNN for audio features\n",
    "        #print(f\"hidden_size : {hidden_size}\")\n",
    "        self.audio_rnn = nn.GRU(audio_feature_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Fusion and final layers\n",
    "        #conv1d\n",
    "        #self.conv1d = nn.Conv1d(1, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(hidden_size + text_feature_size, 2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, audio_features, text_features):\n",
    "        # Process audio features with RNN\n",
    "        #_, audio_x = self.audio_rnn(audio_features)\n",
    "        # Assuming self.audio_rnn is your RNN\n",
    "        audio_rnn_output, _ = self.audio_rnn(audio_features)  # Output shape [batch_size, seq_len, hidden_size]\n",
    "        #print(\"audio_rnn_output shape:\", audio_rnn_output.shape)\n",
    "\n",
    "        #audio_x = audio_rnn_output[:, -1, :]  # Taking the last sequence output, shape [batch_size, hidden_size]\n",
    "\n",
    "        #audio_x = audio_x.squeeze(0)  # If using LSTM, use audio_x[0].squeeze(0)\n",
    "        audio_x=audio_rnn_output\n",
    "        # Process text features\n",
    "        text_x = self.camembert(text_features)[1]\n",
    "        #print(f\"shape text : {text_x.shape}\")\n",
    "        #print(f\"shape audio : {audio_x.shape}\")\n",
    "        # Fuse audio and text features\n",
    "        combined = torch.cat((audio_x, text_x), dim=1)\n",
    "        #couche dense\n",
    "        \n",
    "        # Final layers\n",
    "        #x=self.conv1d(combined.unsqueeze(1))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        #x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x=self.fc1(combined)\n",
    "        #x = self.dropout(F.relu(self.fc2(x)))\n",
    "        #x =self.fc3(x)\n",
    "        return x\n",
    "# Define the dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, audio_features, text_features, labels):\n",
    "        self.audio_features = audio_features\n",
    "        self.text_features = text_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_feature = self.audio_features[idx]\n",
    "        text_feature = self.text_features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convertir les étiquettes en tenseurs de type long\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return audio_feature, text_feature, label\n",
    "\n",
    "\n",
    "# Assuming X_audio, text_features, and y are numpy arrays\n",
    "# Get the size of the audio and text features\n",
    "audio_feature_size = X_audio.shape[1]\n",
    "print(f\"audio_feature_size: {audio_feature_size}\")\n",
    "temp_camembert = CamembertModel.from_pretrained('camembert-base')\n",
    "text_feature_size = temp_camembert.config.hidden_size\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_audio_train, X_audio_test, text_features_train, text_features_test, y_train, y_test = train_test_split(X_audio, text_features, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create DataLoaders for the training and testing sets\n",
    "train_dataset = CombinedDataset(torch.tensor(X_audio_train), torch.tensor(text_features_train), torch.tensor(y_train))\n",
    "test_dataset = CombinedDataset(torch.tensor(X_audio_test), torch.tensor(text_features_test), torch.tensor(y_test))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff8a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = EarlyFusionModel(audio_feature_size, text_feature_size).to(device)\n",
    "\n",
    "# Define class weights for BCEWithLogitsLoss\n",
    "# Increase the weight for the positive class to handle class imbalance\n",
    "w_pos = 10\n",
    "print(\"Weight for positive class:\", w_pos)\n",
    "class_weights = torch.tensor([1.0, w_pos]).to(device)\n",
    "\n",
    "# Use BCEWithLogitsLoss with weight for the positive class\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (audio_inputs, text_inputs, labels) in enumerate(train_loader):\n",
    "        # Move inputs and labels to the device\n",
    "        audio_inputs, text_inputs, labels = audio_inputs.to(device), text_inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(audio_inputs, text_inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    torch.save(model, 'modele/early_model_epoch_'+str(epoch))\n",
    "    # Calculate and print the average loss over the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Initialize counters for accuracy calculation\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Initialize lists to store all predictions and labels for F1 score and confusion matrix\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for audio_inputs, text_inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the device\n",
    "        audio_inputs, text_inputs, labels = audio_inputs.to(device), text_inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(audio_inputs, text_inputs)\n",
    "\n",
    "        # Apply sigmoid function and threshold at 0.5 to get predictions\n",
    "        predicted = torch.sigmoid(outputs).squeeze() > 0.5\n",
    "\n",
    "        # Update counters for accuracy calculation\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for F1 score and confusion matrix\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Calculate and print confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
