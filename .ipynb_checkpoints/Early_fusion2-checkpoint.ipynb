{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import transformers\n",
    "import load_data\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer, CamembertModel, CamembertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import gc\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') # GPU ou CPU\n",
    "print(torch.cuda.is_available())\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CamembertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b345a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CHARGEMENT ET PRÉTRAITEMENT DES DONNÉES TEXTUELLES -------------------\n",
    "\n",
    "transcr_path = 'paco-cheese/transcr'\n",
    "data = load_data.load_all_ipus(folder_path=transcr_path, load_words=True) #fonction donnée par le prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des données en DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extraction de l'information de changement de locuteur\n",
    "y = [0 if i == 0 else 1 for i in range(len(df)) if df['speaker'][i] != df['speaker'][i - 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92979b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bcece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- EXTRACTION DES CARACTÉRISTIQUES AUDIO -------------------\n",
    "audio_files_path = 'paco-cheese/audio/2_channels/'\n",
    "\n",
    "# Cette fonction prend une entrée de dyade et renvoie le chemin du fichier audio correspondant\n",
    "def find_audio_file(dyad,first_speaker):\n",
    "    dyad=dyad.split('\\\\')[1]\n",
    "    #first_speaker=str(first_speaker)\n",
    "    if isinstance(first_speaker, float):\n",
    "        first_speaker=\"NA\"\n",
    "        print(first_speaker)\n",
    "    second_speaker=dyad.replace(first_speaker,\"\")\n",
    "    #if \"LS\" in dyad:\n",
    "    #    print(\"trouvé\")\n",
    "    # Trouver le fichier audio qui contient l'identifiant de la dyade\n",
    "    for file_name in os.listdir(audio_files_path):\n",
    "        #if(\"LS\" in dyad):\n",
    "            #print(first_speaker)\n",
    "            #print(second_speaker)\n",
    "            #print(file_name)\n",
    "        if first_speaker in file_name and second_speaker in file_name:\n",
    "            return os.path.join(audio_files_path, file_name)\n",
    "    return None\n",
    "\n",
    "# Cette fonction extrait les segments audio en utilisant les informations de la dyade et les timestamps\n",
    "def extract_audio_segments(df):\n",
    "    audio_segments = []\n",
    "    audio_file_path = \"\"\n",
    "    nombre_boucle=0\n",
    "    for index, row in df.iterrows():\n",
    "        nombre_boucle+=1\n",
    "        first_speaker=str(row['speaker'])\n",
    "        if isinstance(row['speaker'], float):\n",
    "            first_speaker=\"NA\"\n",
    "            \n",
    "            \n",
    "        if first_speaker not in audio_file_path:\n",
    "            # Si le fichier audio n'a pas encore été chargé, chargez-le\n",
    "            audio = None\n",
    "            gc.collect()\n",
    "            audio_file_path = find_audio_file(row['dyad'],first_speaker)\n",
    "            if audio_file_path is None:\n",
    "                print(\"Audio file not found for dyad {}\".format(row['dyad']))\n",
    "                audio_file_path = \"\"\n",
    "                continue\n",
    "            audio = AudioSegment.from_file(audio_file_path)\n",
    "            #print(index)\n",
    "            print(audio_file_path)  \n",
    "        if audio_file_path!=\"\":\n",
    "            start_ms = int(row['start_ipu'] * 1000)\n",
    "            end_ms = int(row['stop_ipu'] * 1000)\n",
    "            segment = audio[start_ms:end_ms]\n",
    "            audio_segments.append(segment)\n",
    "            # Vous pouvez également enregistrer le segment si nécessaire\n",
    "            # segment.export('segment_{}.wav'.format(index), format='wav')\n",
    "        if audio_file_path==\"\":\n",
    "            print(\"trouvé\")\n",
    "            print(audio_file_path)\n",
    "    return audio_segments,nombre_boucle\n",
    "\n",
    "# Utilisez la fonction pour votre DataFrame\n",
    "audio_segments,nombre_boucle = extract_audio_segments(data)\n",
    "\n",
    "def extract_features(audio_segment):\n",
    "    # Convert PyDub audio segment to numpy array\n",
    "    samples = np.array(audio_segment.get_array_of_samples())\n",
    "\n",
    "    # Normalize the audio samples to floating-point values\n",
    "    if audio_segment.sample_width == 2:\n",
    "        samples = samples.astype(np.float32) / 32768\n",
    "    elif audio_segment.sample_width == 4:\n",
    "        samples = samples.astype(np.float32) / 2147483648\n",
    "\n",
    "    # Use librosa to extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=samples, sr=audio_segment.frame_rate, n_mfcc=13)\n",
    "    \n",
    "    # Average the MFCCs over time\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    return mfccs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078b74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553add03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des caractéristiques MFCC pour tous les segments audio\n",
    "\n",
    "#************************************** A CHANGER A LA FIN\n",
    "audio_segments_small=audio_segments[:1000]\n",
    "X_audio = np.array([extract_features(segment) for segment in audio_segments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "df_save=df\n",
    "X_audio_save=X_audio\n",
    "y_save=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "\n",
    "X=X[:10]\n",
    "y=y[:10]\n",
    "df=df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c04ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- FUSION DES CARACTÉRISTIQUES AUDIO ET TEXTUELLES -------------------\n",
    "\n",
    "# Prétraitement des données textuelles\n",
    "text_features = df['text_words'].fillna('[UNK]')\n",
    "\n",
    "# Utilisation du tokenizer CamemBERT\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "text_features = text_features.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, padding='max_length', truncation=True))\n",
    "\n",
    "# Conversion des caractéristiques textuelles en tensor\n",
    "X_text = torch.tensor(np.array(text_features.tolist()))\n",
    "\n",
    "# Fusion des caractéristiques audio et textuelles\n",
    "X_combined = torch.cat((torch.tensor(X_audio), X_text), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CONSTRUCTION ET ENTRAÎNEMENT DU MODÈLE EN PYTORCH -------------------\n",
    "# Définition du dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        \"\"\"\n",
    "        Initialisation du dataset.\n",
    "        :param features: Les caractéristiques fusionnées (audio et texte).\n",
    "        :param labels: Les étiquettes correspondantes.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retourne la longueur totale du dataset.\n",
    "        \"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Récupère l'échantillon de données et son étiquette à l'index spécifié.\n",
    "        :param idx: Index de l'échantillon à récupérer.\n",
    "        :return: Tuple (échantillon, étiquette).\n",
    "        \"\"\"\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Création des DataLoaders\n",
    "train_dataset = CombinedDataset(X_train, y_train)\n",
    "test_dataset = CombinedDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e26c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données textuelles et extraction des caractéristiques\n",
    "# Assurez-vous que la variable 'data' est chargée comme dans votre script précédent\n",
    "text_features = data['text_words']\n",
    "\n",
    "# Remplacer les valeurs NaN par un placeholder\n",
    "text_features = text_features.fillna('[UNK]')\n",
    "\n",
    "# Utiliser CamemBERT tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "def tokenize_and_pad(text, max_len=256):\n",
    "    return tokenizer.encode(text, max_length=max_len, padding='max_length', truncation=True)\n",
    "\n",
    "# Appliquer le tokenizer à toutes les données textuelles\n",
    "text_features = text_features.apply(lambda x: tokenize_and_pad(str(x)))\n",
    "\n",
    "# Assurez-vous que 'X' est votre matrice de caractéristiques audio extraites\n",
    "audio_features = torch.tensor(X)\n",
    "\n",
    "# Convertir les caractéristiques textuelles en tensor\n",
    "text_features = torch.tensor(np.array(text_features.tolist()))\n",
    "\n",
    "# Fusionner les caractéristiques audio et textuelles\n",
    "combined_features = torch.cat((audio_features, text_features), dim=1)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Créer des datasets et des dataloaders\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = CombinedDataset(X_train, y_train)\n",
    "test_dataset = CombinedDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Définition du modèle de réseau de neurones\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = EarlyFusionModel(X_train.shape[1])\n",
    "\n",
    "# Entraînement du modèle\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Évaluation du modèle\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c52a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18072f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe5826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b6eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33712f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab40154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bac7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4c5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78f31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75510031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee6534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc93688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd69e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09100c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabb316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988d6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8de29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
