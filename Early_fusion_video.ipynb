{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertModel, BertTokenizer, CamembertModel, CamembertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "# Local application imports\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the transcription files\n",
    "transcr_path = 'paco-cheese/transcr'\n",
    "\n",
    "# Load and preprocess all the Inter-Pausal Units (IPUs) from the transcription files\n",
    "data = load_data.load_all_ipus(folder_path=transcr_path, load_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a new list 'y' to store 1s and 0s based on speaker change\n",
    "y = [0]  # Initialize the first element to 0, as there's no previous speaker to compare with.\n",
    "\n",
    "# Iterate over the DataFrame, starting from the second row\n",
    "for i in range(0, len(df)-1):\n",
    "    # Check if the current speaker is different from the previous one\n",
    "    if df['speaker'][i] != df['speaker'][i+1]:\n",
    "        y.append(1)  # Speaker changed\n",
    "    else:\n",
    "        y.append(0)  # Speaker did not change\n",
    "\n",
    "# Print the list 'y'\n",
    "y\n",
    "\n",
    "# Get the indices of the speaker changes\n",
    "indices = [i for i, x in enumerate(y) if x == 1]\n",
    "\n",
    "# Print the number of speaker changes and their indices\n",
    "print(len(indices))\n",
    "print(indices)\n",
    "\n",
    "# Print the speakers at the indices of speaker changes\n",
    "for i in indices:\n",
    "    print(df['speaker'][i])\n",
    "\n",
    "# Display the last 5 rows of the DataFrame\n",
    "df[len(indices)-5:]\n",
    "\n",
    "# Filter the DataFrame for rows where the speaker is 'LS'\n",
    "ls = df[df['speaker']=='LS']\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "ls\n",
    "\n",
    "# Check if there are any rows where the speaker is NaN\n",
    "df[df['speaker'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_save=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame 'df_save' to include only the rows where the 'dyad' column is 'transcr\\AAOR'\n",
    "df = df_save[df_save['dyad'] == 'transcr\\\\AAOR']\n",
    "\n",
    "# Print the shape of the filtered DataFrame to see the number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67323d",
   "metadata": {},
   "source": [
    "# Process videos\n",
    "\n",
    "### obtenirs les paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01566240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the video file for a given dyad and first speaker\n",
    "def find_video_file(dyad, first_speaker):\n",
    "    # Extract the dyad name from the path\n",
    "    dyad = dyad.split('\\\\')[1]\n",
    "    \n",
    "    # If the first speaker is NaN, replace it with \"NA\"\n",
    "    if isinstance(first_speaker, float):\n",
    "        first_speaker = \"NA\"\n",
    "\n",
    "    # Determine the second speaker by removing the first speaker from the dyad name\n",
    "    second_speaker = dyad.replace(first_speaker, \"\")\n",
    "\n",
    "    # Define the subdirectories to search in\n",
    "    subdirs = ['cheese', 'paco']\n",
    "    \n",
    "    # Loop over the subdirectories\n",
    "    for subdir in subdirs:\n",
    "        # Define the path to the media files\n",
    "        media_files_path = f'paco-cheese/video/video/{subdir}/'\n",
    "        \n",
    "        # Loop over the files in the media files path\n",
    "        for file_name in os.listdir(media_files_path):\n",
    "            # If the file name contains both the first and second speaker, return the file path\n",
    "            if first_speaker in file_name and second_speaker in file_name:\n",
    "                return os.path.join(media_files_path, file_name)\n",
    "\n",
    "    # If no file is found, return None\n",
    "    return None\n",
    "\n",
    "# Loop over the rows in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the first speaker from the row, or \"NA\" if it's NaN\n",
    "    first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "    \n",
    "    # Find the video file for the dyad and first speaker\n",
    "    video_path = find_video_file(row['dyad'], first_speaker)\n",
    "\n",
    "    # Print the video path\n",
    "    print(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ce75f",
   "metadata": {},
   "source": [
    "### Extraires les differentes features des videos :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract segments from a video\n",
    "def extract_video_segments(video_file_path, start_ms, end_ms):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    # Initialize a list to store the frames\n",
    "    frames = []\n",
    "    # Get the frames per second (fps) of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # Calculate the start and end frames\n",
    "    start_frame = int(start_ms * fps / 1000)\n",
    "    end_frame = int(end_ms * fps / 1000)\n",
    "    \n",
    "    # Initialize the current frame number\n",
    "    current_frame = 0\n",
    "    # Loop over the frames in the video\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        # If the frame could not be read or the current frame is past the end frame, break the loop\n",
    "        if not ret or current_frame > end_frame:\n",
    "            break\n",
    "        # If the current frame is within the segment, append it to the list\n",
    "        if current_frame >= start_frame:\n",
    "            frames.append(frame)\n",
    "        # Increment the current frame number\n",
    "        current_frame += 1\n",
    "    # Release the video file\n",
    "    cap.release()\n",
    "    # Return the list of frames\n",
    "    return frames\n",
    "\n",
    "# Define a function to preprocess a frame\n",
    "def preprocess_frame(frame):\n",
    "    # Resize the frame to 224x224 pixels for VGG16\n",
    "    frame_resized = cv2.resize(frame, (224, 224))\n",
    "    return frame_resized\n",
    "\n",
    "# Define a class for feature extraction\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # Load the VGG16 model with pre-trained weights\n",
    "        base_model = VGG16(weights='imagenet')\n",
    "        # Create a new model that outputs the features from the 'fc1' layer of the base model\n",
    "        self.model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc1').output)\n",
    "\n",
    "    def extract(self, frame):\n",
    "        # Expand the dimensions of the frame\n",
    "        frame = np.expand_dims(frame, axis=0)\n",
    "        # Preprocess the frame\n",
    "        frame = preprocess_input(frame)\n",
    "        # Extract the features from the frame\n",
    "        features = self.model.predict(frame)\n",
    "        # Remove the extra dimension from the features\n",
    "        return features.squeeze()\n",
    "\n",
    "# Define a function to fuse the features\n",
    "def fuse_features(features):\n",
    "    # If there are any features, return their mean; otherwise, return a zero vector\n",
    "    return np.mean(features, axis=0) if features else np.zeros((512,))\n",
    "\n",
    "# Initialize the feature extractor\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Initialize a list to store the video features\n",
    "X_video = []\n",
    "\n",
    "# Loop over the rows in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the first speaker from the row, or \"NA\" if it's NaN\n",
    "    first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "    # Find the video file for the dyad and first speaker\n",
    "    video_file_path = find_video_file(row['dyad'], first_speaker)\n",
    "\n",
    "    # If the video file was found\n",
    "    if video_file_path:\n",
    "        print(f\"Processing video file: {video_file_path}\")\n",
    "        # Calculate the start and end times in milliseconds\n",
    "        start_ms = int(row['start_ipu'] * 1000)\n",
    "        end_ms = int(row['stop_ipu'] * 1000)\n",
    "        # Extract the frames from the video segment\n",
    "        frames = extract_video_segments(video_file_path, start_ms, end_ms)\n",
    "\n",
    "        # If no frames were extracted\n",
    "        if not frames:\n",
    "            print(\"No frames extracted from video file.\")\n",
    "            X_video.append(np.zeros((512,)))\n",
    "            continue\n",
    "        # Display each frame\n",
    "        for frame in frames:\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"Number of frames extracted: {len(frames)}\")\n",
    "        # Preprocess each frame\n",
    "        frames = [preprocess_frame(frame) for frame in frames]\n",
    "        # Extract the features from each frame\n",
    "        features = extract_video_features(frames, feature_extractor)\n",
    "        # Fuse the features\n",
    "        video_features = fuse_features(features)\n",
    "        # Append the features to the list\n",
    "        X_video.append(video_features)\n",
    "        print(f\"Video features extracted: {video_features}\")\n",
    "    else:\n",
    "        print(f\"No video file path found for {row['dyad']} and {first_speaker}\")\n",
    "        X_video.append(np.zeros((512,)))\n",
    "\n",
    "# Convert the list of video features to a NumPy array\n",
    "X_video = np.array(X_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e45291",
   "metadata": {},
   "source": [
    "# Process des audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c779c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the audio files\n",
    "audio_files_path = 'paco-cheese/audio/2_channels/'\n",
    "\n",
    "# Define a function to find the audio file for a given dyad and first speaker\n",
    "def find_audio_file(dyad, first_speaker):\n",
    "    # Extract the dyad name from the path\n",
    "    dyad = dyad.split('\\\\')[1]\n",
    "\n",
    "    #first_speaker=str(first_speaker)\n",
    "\n",
    "    # If the first speaker is NaN, replace it with \"NA\"\n",
    "    if isinstance(first_speaker, float):\n",
    "        first_speaker = \"NA\"\n",
    "    # Determine the second speaker by removing the first speaker from the dyad name\n",
    "    second_speaker = dyad.replace(first_speaker, \"\")\n",
    "    # Loop over the files in the audio files path\n",
    "    for file_name in os.listdir(audio_files_path):\n",
    "        # If the file name contains both the first and second speaker, return the file path\n",
    "        if first_speaker in file_name and second_speaker in file_name:\n",
    "            return os.path.join(audio_files_path, file_name)\n",
    "    # If no file is found, return None\n",
    "    return None\n",
    "\n",
    "# Define a function to extract audio segments from a DataFrame\n",
    "def extract_audio_segments(df):\n",
    "    # Initialize a list to store the audio segments\n",
    "    audio_segments = []\n",
    "    # Initialize the audio file path\n",
    "    audio_file_path = \"\"\n",
    "    # Loop over the rows in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Get the first speaker from the row, or \"NA\" if it's NaN\n",
    "        first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "        # If the first speaker is not in the audio file path\n",
    "        if first_speaker not in audio_file_path:\n",
    "            # If the audio file has not been loaded yet, load it\n",
    "            audio = None\n",
    "            gc.collect()\n",
    "            audio_file_path = find_audio_file(row['dyad'], first_speaker)\n",
    "            if audio_file_path is None:\n",
    "                print(\"Audio file not found for dyad {}\".format(row['dyad']))\n",
    "                audio_file_path = \"\"\n",
    "                continue\n",
    "            audio = AudioSegment.from_file(audio_file_path)\n",
    "        # If the audio file path is not empty\n",
    "        if audio_file_path != \"\":\n",
    "            # Calculate the start and end times in milliseconds\n",
    "            start_ms = int(row['start_ipu'] * 1000)\n",
    "            end_ms = int(row['stop_ipu'] * 1000)\n",
    "            # Extract the audio segment\n",
    "            segment = audio[start_ms:end_ms]\n",
    "            # Append the segment to the list\n",
    "            audio_segments.append(segment)\n",
    "    # Return the list of audio segments\n",
    "    return audio_segments\n",
    "\n",
    "# Use the function to extract audio segments from your DataFrame\n",
    "audio_segments = extract_audio_segments(data)\n",
    "\n",
    "# Define a function to extract features from an audio segment\n",
    "def extract_features(audio_segment):\n",
    "    # Convert the PyDub audio segment to a numpy array\n",
    "    samples = np.array(audio_segment.get_array_of_samples())\n",
    "    # Normalize the audio samples to floating-point values\n",
    "    if audio_segment.sample_width == 2:\n",
    "        samples = samples.astype(np.float32) / 32768\n",
    "    elif audio_segment.sample_width == 4:\n",
    "        samples = samples.astype(np.float32) / 2147483648\n",
    "    # Use librosa to extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=samples, sr=audio_segment.frame_rate, n_mfcc=13)\n",
    "    # Average the MFCCs over time\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    # Return the mean MFCCs\n",
    "    return mfccs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f76990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display\n",
    "print(len(audio_segments))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "# Extract in a list the MFCC features for all audio segments, and convert the list to a numpy array\n",
    "X_audio = np.array([extract_features(segment) for segment in audio_segments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1944fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "# Save the current DataFrame 'df'\n",
    "df_save = df\n",
    "\n",
    "# Save the current audio features array 'X_audio'\n",
    "X_audio_save = X_audio\n",
    "\n",
    "# Save the current target variable array 'y'\n",
    "y_save = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840163bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************** A CHANGER A LA FIN\n",
    "\n",
    "# Set a limit for the number of samples to use for testing\n",
    "limit = 1000  #107603  #110544\n",
    "X_audio = X_audio_save[:limit]\n",
    "y = y_save[:limit]\n",
    "df = df_save[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc9299",
   "metadata": {},
   "source": [
    "# Nouveau modele, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51233769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the textual data and extract the features\n",
    "# Make sure the 'data' variable is loaded as in your previous script\n",
    "text_features = data['text_words'][:limit]\n",
    "\n",
    "# Replace NaN values with a placeholder\n",
    "text_features = text_features.fillna('[UNK]')\n",
    "\n",
    "# Use CamemBERT tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Define a function to tokenize and pad the text to a maximum length of 256\n",
    "def tokenize_and_pad(text, max_len=256):\n",
    "    return tokenizer.encode(text, max_length=max_len, padding='max_length', truncation=True)\n",
    "\n",
    "# Apply the tokenizer to all textual data\n",
    "text_features = text_features.apply(lambda x: tokenize_and_pad(str(x)))\n",
    "\n",
    "# Make sure 'X' is your extracted audio features matrix\n",
    "audio_features = torch.tensor(X_audio)\n",
    "\n",
    "# Convert the textual features into a tensor\n",
    "text_features = torch.tensor(np.array(text_features.tolist()))\n",
    "\n",
    "# Merge the audio and textual features\n",
    "combined_features = torch.cat((audio_features, text_features), dim=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc94122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the class distribution in the training and test sets\n",
    "\n",
    "# Count the number of each class for y_train and y_test\n",
    "train_class_distribution = pd.Series(y_train).value_counts()\n",
    "test_class_distribution = pd.Series(y_test).value_counts()\n",
    "\n",
    "# Display\n",
    "print(\"Distribution of classes in the training set:\")\n",
    "print(train_class_distribution)\n",
    "print(\"Distribution of classes in the test set:\")\n",
    "print(test_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an early fusion model for audio and text features\n",
    "class EarlyFusionModel(nn.Module):\n",
    "    def __init__(self, audio_feature_size, text_feature_size, dropout_rate=0.5):\n",
    "        super(EarlyFusionModel, self).__init__()\n",
    "        # CamemBERT for text features\n",
    "        self.camembert = CamembertModel.from_pretrained('camembert-base')\n",
    "        # Layers for audio features\n",
    "        self.audio_fc1 = nn.Linear(audio_feature_size, 128)\n",
    "        self.audio_fc2 = nn.Linear(128, 64)\n",
    "        # Fusion and final layers\n",
    "        self.fc1 = nn.Linear(64 + text_feature_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, audio_features, text_features):\n",
    "        # Process audio features\n",
    "        audio_x = F.relu(self.audio_fc1(audio_features))\n",
    "        audio_x = F.relu(self.audio_fc2(audio_x))\n",
    "        # Process text features\n",
    "        text_x = self.camembert(text_features)[1]\n",
    "        # Fuse audio and text features\n",
    "        combined = torch.cat((audio_x, text_x), dim=1)\n",
    "        # Final layers\n",
    "        x = self.dropout(F.relu(self.fc1(combined)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define a dataset for combined audio and text features\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, audio_features, text_features, labels):\n",
    "        self.audio_features = audio_features\n",
    "        self.text_features = text_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio_features[idx], self.text_features[idx], self.labels[idx]\n",
    "\n",
    "# Determine the size of the audio and text features\n",
    "audio_feature_size = X_audio.shape[1]\n",
    "temp_camembert = CamembertModel.from_pretrained('camembert-base')\n",
    "text_feature_size = temp_camembert.config.hidden_size\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_audio_train, X_audio_test, text_features_train, text_features_test, y_train, y_test = train_test_split(X_audio, text_features, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create DataLoaders for the training and test sets\n",
    "train_dataset = CombinedDataset(torch.tensor(X_audio_train), torch.tensor(text_features_train), torch.tensor(y_train))\n",
    "test_dataset = CombinedDataset(torch.tensor(X_audio_test), torch.tensor(text_features_test), torch.tensor(y_test))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff8a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the early fusion model and move it to the GPU if available\n",
    "model = EarlyFusionModel(audio_feature_size, text_feature_size).to(device)\n",
    "\n",
    "# Define the weights for the positive and negative classes for the loss function\n",
    "# Currently, the weight for the positive class is set to 1 for testing purposes\n",
    "w_pos = 1\n",
    "class_weights = torch.tensor([1.0, w_pos]).to(device)\n",
    "\n",
    "# Define the loss function, using Binary Cross Entropy with Logits Loss\n",
    "# The weight for the positive class is applied\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "# Define the optimizer, using Adam with a learning rate of 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (audio_inputs, text_inputs, labels) in enumerate(train_loader):\n",
    "        # Move the inputs and labels to the GPU if available\n",
    "        audio_inputs, text_inputs, labels = audio_inputs.to(device), text_inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(audio_inputs, text_inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss over the epoch\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate and print the average loss over the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "# Initialize counters and lists for accuracy, F1 score, and confusion matrix calculation\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# No gradient calculation during evaluation\n",
    "with torch.no_grad():\n",
    "    for audio_inputs, text_inputs, labels in test_loader:\n",
    "        # Move the inputs and labels to the GPU if available\n",
    "        audio_inputs, text_inputs, labels = audio_inputs.to(device), text_inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(audio_inputs, text_inputs)\n",
    "\n",
    "        # Apply a sigmoid activation to the outputs and threshold at 0.5 for binary classification\n",
    "        predicted = torch.sigmoid(outputs).squeeze() > 0.5\n",
    "\n",
    "        # Update the counters for accuracy calculation\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Append the predictions and labels for F1 score and confusion matrix calculation\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Calculate and print the F1 score\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Calculate and print the confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
