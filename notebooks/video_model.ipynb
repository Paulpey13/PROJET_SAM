{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2944723",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e77bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610dd26",
   "metadata": {},
   "source": [
    "### Import les modules fait maison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481026f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "src = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(src)\n",
    "import src.video.video_extract as ve\n",
    "import src.video.video_model as vm\n",
    "import src.load_data as load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59091062",
   "metadata": {},
   "source": [
    "### Chargement du dataset (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1072d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcr_path = '../paco-cheese/transcr'\n",
    "data = load_data.load_all_ipus(folder_path=transcr_path, load_words=True) #fonction donnée par le prof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65971e2",
   "metadata": {},
   "source": [
    "### Réduire le dataset pour réaliser des test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df_save=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6192469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour avoir juste une video et raccourcir df (test)\n",
    "df=df_save[df_save['dyad']=='transcr\\MAPC'][0:2] #pour prendre peu de temps sinon vraiment trop lourd et long\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7168183",
   "metadata": {},
   "source": [
    "### Fonction `find_video_file` permettant de synchroniser les vidéos à leur ligne dans le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_files_path = 'paco-cheese/video/video/'\n",
    "#trouver les path des videos\n",
    "def find_video_file(dyad, first_speaker):\n",
    "    dyad = dyad.split('\\\\')[1]\n",
    "    if isinstance(first_speaker, float):\n",
    "        first_speaker = \"NA\"\n",
    "\n",
    "    second_speaker = dyad.replace(first_speaker, \"\")\n",
    "\n",
    "    subdirs = ['cheese', 'paco']\n",
    "    for subdir in subdirs:\n",
    "        media_files_path = f'../paco-cheese/video/video/{subdir}/'\n",
    "        for file_name in os.listdir(media_files_path):\n",
    "            if first_speaker in file_name and second_speaker in file_name:\n",
    "                return os.path.join(media_files_path, file_name)\n",
    "\n",
    "    return None\n",
    "for index, row in df.iterrows():\n",
    "    first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "    video_path = find_video_file(row['dyad'], first_speaker)\n",
    "\n",
    "    print(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954ce20",
   "metadata": {},
   "source": [
    "### Utilisation du module video_extract pour extraire les features des differentes vidéos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700bf57",
   "metadata": {},
   "source": [
    "### Plus d'explications sur le module d'extraction :\n",
    "\n",
    "Le code ci-dessus implémente un processus pour extraire, traiter et analyser des caractéristiques à partir de vidéos :\n",
    "\n",
    "1. **Extraction des segments vidéo** : La fonction `extract_video_segments` utilise OpenCV pour extraire des frames d'une vidéo entre des timestamps spécifiés (de `start_ms` à `end_ms`). Elle utilise `cv2.VideoCapture` pour lire la vidéo et extrait les frames pertinentes en fonction du framerate de la vidéo (`fps`).\n",
    "\n",
    "2. **Prétraitement des frames** : `preprocess_frame` redimensionne chaque frame à une taille fixe (224x224), nécessaire pour des modèles de réseaux de neurones comme VGG16 (qu'on utilisait précédemment, on a décider de garder cette taille).\n",
    "\n",
    "3. **Extraction de caractéristiques avec FeatureExtractor** :\n",
    "    - Initialise un modèle de détection de visages (`face_net`) et un modèle de landmarks faciaux (`landmark_model`).\n",
    "    - La méthode `extract` détecte les visages dans une frame, puis extrait et dessine les landmarks faciaux. Pour chaque détection réussie, elle collecte les keypoints des landmarks faciaux.\n",
    "\n",
    "4. **Fusion des caractéristiques** : La fonction `fuse_features` permet de combiner les caractéristiques extraites de différentes manières selon les besoins spécifiques de l'application.\n",
    "\n",
    "5. **Extraction et traitement des caractéristiques vidéo** :\n",
    "    - Boucle sur chaque enregistrement dans un DataFrame, extrait le chemin du fichier vidéo, et pour chaque vidéo, extrait les frames, les prétraite, et extrait les caractéristiques avec `FeatureExtractor`.\n",
    "    - Les caractéristiques de la vidéo sont ensuite fusionnées et stockées dans `X_video`.\n",
    "\n",
    "Ce processus est conçu pour transformer des segments vidéo en un ensemble de caractéristiques prêtes à être analysées ou utilisées pour l'entraînement de modèles de machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133fe32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = ve.FeatureExtractor()\n",
    "\n",
    "X_video = []\n",
    "\n",
    "# count=0\n",
    "for index, row in df.iterrows():\n",
    "    # count=count+1\n",
    "    # if count==5:\n",
    "    #     break;\n",
    "    first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "    video_file_path = find_video_file(row['dyad'], first_speaker)\n",
    "    #video_file_path=\"test_2.mp4\"\n",
    "    if video_file_path:\n",
    "        print(f\"Traitement du fichier vidéo : {video_file_path}\")\n",
    "        start_ms = int(row['start_ipu'] * 1000)\n",
    "        end_ms = int(row['stop_ipu'] * 1000)\n",
    "        frames = ve.extract_video_segments(video_file_path, start_ms, end_ms)  # Using ve.extract_video_segments\n",
    "\n",
    "        if not frames:\n",
    "            print(\"Aucun cadre extrait du fichier vidéo.\")\n",
    "            X_video.append(np.zeros((512,)))\n",
    "            continue\n",
    "        for frame in frames:\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"Nombre de cadres extraits : {len(frames)}\")\n",
    "        features = []\n",
    "        for frame in frames:\n",
    "            frame_preprocessed = ve.preprocess_frame(frame)  # Using ve.preprocess_frame\n",
    "            keypoints = feature_extractor.extract(frame_preprocessed)\n",
    "            features.extend(keypoints)  # Concaténation des keypoints de tous les cadres\n",
    "        video_features = ve.fuse_features(features)  # Using ve.fuse_features\n",
    "        X_video.append(video_features)\n",
    "        print(f\"Caractéristiques vidéo extraites : {len(video_features)} points détectés.\")\n",
    "    else:\n",
    "        print(f\"Aucun chemin de fichier vidéo trouvé pour {row['dyad']} et {first_speaker}\")\n",
    "        X_video.append(np.zeros((512,)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46ec21",
   "metadata": {},
   "source": [
    "# Modèle\n",
    "\n",
    "\n",
    "### Plus d'informations sur le module dataset et modele :\n",
    "\n",
    "### Implémentation d'un Modèle LSTM pour l'Analyse Vidéo\n",
    "\n",
    "Notre projet a également exploré l'intégration de l'analyse vidéo. Bien que notre attention principale soit orientée vers l'audio et le texte, nous avons développé un modèle LSTM basique pour démontrer notre capacité à traiter et analyser les caractéristiques extraites des vidéos.\n",
    "\n",
    "#### Préparation des Données\n",
    "- **Padding des séquences vidéo** : Nous avons normalisé la longueur des séquences vidéo en utilisant `pad_sequence` pour que toutes aient la même taille, ce qui est une étape essentielle pour le traitement par LSTM.\n",
    "- **Aplatissage des Caractéristiques** : Les caractéristiques vidéo extraites ont été remodelées pour s'adapter à l'entrée 3D attendue par le LSTM (batch, seq_len, features).\n",
    "\n",
    "#### Modèle LSTM\n",
    "- **Conception** : Le modèle `LSTMClassifier` est une architecture LSTM, avec la possibilité d'ajuster le nombre de couches via `num_layers`. Cela permet de moduler la complexité du modèle en fonction de la nature des données.\n",
    "- **Optimisation** : Le modèle utilise une fonction de perte BCE et un optimiseur Adam, des choix courants pour les tâches de classification binaire.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Par manque de temps, nous avons décidé de nous concentrer sur l'audio et le texte car l'entraînement et l'extraction de features avec des vidéos peut s'avérer très long et nos ordinateurs peu adaptés.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75588395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding de X_video pour que toutes les séquences aient la même longueur\n",
    "padded_X_video = pad_sequence([torch.tensor(sequence) for sequence in X_video], batch_first=True, padding_value=0)\n",
    "\n",
    "# Aplatir les deux dernières dimensions pour obtenir une entrée 3D et être dapté au LSTM (batch, seq_len, features)\n",
    "padded_X_video = padded_X_video.view(padded_X_video.shape[0], padded_X_video.shape[1], -1)\n",
    "\n",
    "# Conversion de df['turn_after'] en tensor\n",
    "y_tensor = torch.tensor(df['turn_after'].values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X_video, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer les data loaders\n",
    "train_data = DataLoader(list(zip(X_train, y_train)), batch_size=2, shuffle=True)\n",
    "test_data = DataLoader(list(zip(X_test, y_test)), batch_size=2, shuffle=True)\n",
    "\n",
    "train_data = DataLoader(list(zip(X_train, y_train)), batch_size=2, shuffle=True)\n",
    "test_data = DataLoader(list(zip(X_test, y_test)), batch_size=2, shuffle=True)\n",
    "\n",
    "# Parametres\n",
    "embedding_dim = padded_X_video.shape[2]\n",
    "hidden_dim = 64\n",
    "label_size = 1\n",
    "num_layers = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Creer le model (sur CPU si dispo ou voulu)\n",
    "model = vm.LSTMClassifier(embedding_dim, hidden_dim, label_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Loss et optimizer\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrainement\n",
    "vm.train_model(model, train_data, loss_function, optimizer, num_epochs=10, device=device)\n",
    "\n",
    "# Evaluation\n",
    "test_labels, test_predictions = vm.evaluate_model(model, test_data, device=device)\n",
    "\n",
    "# Ici on applique un seuil pour pouvoir mieux prédire (fait office de poids sur la classe positive, ça change un peu des autres modèles)\n",
    "test_predictions = [1 if prob > 0.5 else 0 for prob in test_predictions]\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0400a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c81bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b54c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01927a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce924e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44642715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9a0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65e93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d2cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca95137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6217c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f8158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac14a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d01a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d312d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516707fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3be045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc76cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d167eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5e3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71690599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1411020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c964d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9cc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe334da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7d1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
