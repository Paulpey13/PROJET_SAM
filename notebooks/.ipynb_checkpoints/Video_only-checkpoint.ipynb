{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6b39e1",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import transformers\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer, CamembertModel, CamembertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import gc\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#Import les file de src\n",
    "import os\n",
    "import sys\n",
    "src = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(src)\n",
    "from src.pipeline_audio_model import *\n",
    "import src.load_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') # GPU ou CPU\n",
    "print(torch.cuda.is_available())\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CamembertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdce5e3",
   "metadata": {},
   "source": [
    "### Load data and create label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CHARGEMENT ET PRÉTRAITEMENT DES DONNÉES TEXTUELLES -------------------\n",
    "\n",
    "transcr_path = '../paco-cheese/transcr'\n",
    "data = load_data.load_all_ipus(folder_path=transcr_path, load_words=True) #fonction donnée par le prof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f834694",
   "metadata": {},
   "source": [
    "### Create label (previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a new list 'y' to store 1s and 0s based on speaker change\n",
    "y = [0]  # Initialize the first element to 0, as there's no previous speaker to compare with.\n",
    "\n",
    "# Iterate over the DataFrame, starting from the second row\n",
    "for i in range(0, len(df)-1):\n",
    "    # Check if the current speaker is different from the previous one\n",
    "    if df['speaker'][i] != df['speaker'][i+1]:\n",
    "        y.append(1)  # Speaker changed\n",
    "    else:\n",
    "        y.append(0)  # Speaker did not change\n",
    "\n",
    "y\n",
    "indices = [i for i, x in enumerate(y) if x == 1]\n",
    "print(len(indices))\n",
    "print(indices)\n",
    "#affiche speaker différent dans df\n",
    "\n",
    "for i in indices:\n",
    "    print(df['speaker'][i])\n",
    "\n",
    "\n",
    "df[len(indices)-5:]\n",
    "#look for speaker LS in df\n",
    "ls=df[df['speaker']=='LS']\n",
    "ls\n",
    "#check si un speaker est a nan:\n",
    "df[df['speaker'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour avoir juste une video et raccourcir df (test)\n",
    "df=df_save[df_save['dyad']=='transcr\\MAPC'][0:100] #pour prendre peu de temps sinon vraiment trop lourd et long\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67323d",
   "metadata": {},
   "source": [
    "# Fonction `find_video_file` permettant de synchroniser les vidéos à leur ligne dans le dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01566240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_files_path = 'paco-cheese/video/video/'\n",
    "#trouver les path des videos\n",
    "def find_video_file(dyad, first_speaker):\n",
    "    dyad = dyad.split('\\\\')[1]\n",
    "    if isinstance(first_speaker, float):\n",
    "        first_speaker = \"NA\"\n",
    "\n",
    "    second_speaker = dyad.replace(first_speaker, \"\")\n",
    "\n",
    "    subdirs = ['cheese', 'paco']\n",
    "    for subdir in subdirs:\n",
    "        media_files_path = f'../paco-cheese/video/video/{subdir}/'\n",
    "        for file_name in os.listdir(media_files_path):\n",
    "            if first_speaker in file_name and second_speaker in file_name:\n",
    "                return os.path.join(media_files_path, file_name)\n",
    "\n",
    "    return None\n",
    "for index, row in df.iterrows():\n",
    "    first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "    video_path = find_video_file(row['dyad'], first_speaker)\n",
    "\n",
    "    print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b991fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4ce75f",
   "metadata": {},
   "source": [
    "Le code ci-dessus implémente un processus pour extraire, traiter et analyser des caractéristiques à partir de vidéos :\n",
    "\n",
    "1. **Extraction des segments vidéo** : La fonction `extract_video_segments` utilise OpenCV pour extraire des frames d'une vidéo entre des timestamps spécifiés (de `start_ms` à `end_ms`). Elle utilise `cv2.VideoCapture` pour lire la vidéo et extrait les frames pertinentes en fonction du framerate de la vidéo (`fps`).\n",
    "\n",
    "2. **Prétraitement des frames** : `preprocess_frame` redimensionne chaque frame à une taille fixe (224x224), nécessaire pour des modèles de réseaux de neurones comme VGG16 (qu'on utilisait précédemment, on a décider de garder cette taille).\n",
    "\n",
    "3. **Extraction de caractéristiques avec FeatureExtractor** :\n",
    "    - Initialise un modèle de détection de visages (`face_net`) et un modèle de landmarks faciaux (`landmark_model`).\n",
    "    - La méthode `extract` détecte les visages dans une frame, puis extrait et dessine les landmarks faciaux. Pour chaque détection réussie, elle collecte les keypoints des landmarks faciaux.\n",
    "\n",
    "4. **Fusion des caractéristiques** : La fonction `fuse_features` permet de combiner les caractéristiques extraites de différentes manières selon les besoins spécifiques de l'application.\n",
    "\n",
    "5. **Extraction et traitement des caractéristiques vidéo** :\n",
    "    - Boucle sur chaque enregistrement dans un DataFrame, extrait le chemin du fichier vidéo, et pour chaque vidéo, extrait les frames, les prétraite, et extrait les caractéristiques avec `FeatureExtractor`.\n",
    "    - Les caractéristiques de la vidéo sont ensuite fusionnées et stockées dans `X_video`.\n",
    "\n",
    "Ce processus est conçu pour transformer des segments vidéo en un ensemble de caractéristiques prêtes à être analysées ou utilisées pour l'entraînement de modèles de machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction pour extraire des segments de la vidéo\n",
    "def extract_video_segments(video_file_path, start_ms, end_ms):\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    frames = []\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(start_ms * fps / 1000)\n",
    "    end_frame = int(end_ms * fps / 1000)\n",
    "    \n",
    "    current_frame = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or current_frame > end_frame:\n",
    "            break\n",
    "        if current_frame >= start_frame:\n",
    "            frames.append(frame)\n",
    "        current_frame += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Fonction de prétraitement de chaque cadre\n",
    "def preprocess_frame(frame):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))  # Redimensionnement pour VGG16\n",
    "    return frame_resized\n",
    "\n",
    "# Classe pour l'extraction de caractéristiques\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, min_num_of_points=68):\n",
    "        modelFile = \"model_video/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "        configFile = \"model_video/deploy.prototxt\"\n",
    "        self.face_net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
    "        self.landmark_model = cv2.face.createFacemarkLBF()\n",
    "        model_path = 'lbfmodel.yaml'\n",
    "        self.landmark_model.loadModel(model_path)\n",
    "        self.expected_num_of_points = min_num_of_points\n",
    "\n",
    "    def extract(self, frame):\n",
    "        h, w = frame.shape[:2]  # Définir la hauteur et la largeur de l'image\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False)\n",
    "        self.face_net.setInput(blob)\n",
    "        detections = self.face_net.forward()\n",
    "        all_keypoints = []\n",
    "\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.7:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces_array = np.array([[startX, startY, endX-startX, endY-startY]], dtype=np.int32)\n",
    "                success, landmarks = self.landmark_model.fit(gray, faces_array)\n",
    "                if success and landmarks is not None:\n",
    "                    for landmark_set in landmarks:\n",
    "                        for landmark in landmark_set:\n",
    "                            for point in landmark:\n",
    "                                x, y = int(point[0]), int(point[1])\n",
    "                                cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "                        all_keypoints.extend(landmark_set)\n",
    "\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        return all_keypoints\n",
    "\n",
    "# Fonction pour fusionner les caractéristiques\n",
    "def fuse_features(features):\n",
    "    # Vous devrez décider de la logique de fusion selon vos besoins\n",
    "    return features if features else [np.zeros((68, 2))]  # Exemple: retourne une liste de points zéros si vide\n",
    "\n",
    "# Initialisation de l'extracteur de caractéristiques\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "X_video = []\n",
    "\n",
    "# count=0\n",
    "for index, row in df.iterrows():\n",
    "#     count=count+1\n",
    "#     if count==5:\n",
    "#         break;\n",
    "    first_speaker = str(row['speaker']) if not pd.isna(row['speaker']) else \"NA\"\n",
    "    video_file_path = find_video_file(row['dyad'], first_speaker)\n",
    "    #video_file_path=\"test_2.mp4\"\n",
    "    if video_file_path:\n",
    "        print(f\"Traitement du fichier vidéo : {video_file_path}\")\n",
    "        start_ms = int(row['start_ipu'] * 1000)\n",
    "        end_ms = int(row['stop_ipu'] * 1000)\n",
    "        frames = extract_video_segments(video_file_path, start_ms, end_ms)\n",
    "\n",
    "        if not frames:\n",
    "            print(\"Aucun cadre extrait du fichier vidéo.\")\n",
    "            X_video.append(np.zeros((512,)))\n",
    "            continue\n",
    "        for frame in frames:\n",
    "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"Nombre de cadres extraits : {len(frames)}\")\n",
    "        features = []\n",
    "        for frame in frames:\n",
    "            frame_preprocessed = preprocess_frame(frame)\n",
    "            keypoints = feature_extractor.extract(frame_preprocessed)\n",
    "            features.extend(keypoints)  # Concaténation des keypoints de tous les cadres\n",
    "        video_features = fuse_features(features)\n",
    "        X_video.append(video_features)\n",
    "        print(f\"Caractéristiques vidéo extraites : {len(video_features)} points détectés.\")\n",
    "    else:\n",
    "        print(f\"Aucun chemin de fichier vidéo trouvé pour {row['dyad']} et {first_speaker}\")\n",
    "        X_video.append(np.zeros((512,)))\n",
    "\n",
    "# X_video = np.array(X_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471162a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d03260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7a4eab0",
   "metadata": {},
   "source": [
    "### Implémentation d'un Modèle LSTM pour l'Analyse Vidéo\n",
    "\n",
    "Notre projet a également exploré l'intégration de l'analyse vidéo. Bien que notre attention principale soit orientée vers l'audio et le texte, nous avons développé un modèle LSTM basique pour démontrer notre capacité à traiter et analyser les caractéristiques extraites des vidéos.\n",
    "\n",
    "#### Préparation des Données\n",
    "- **Padding des séquences vidéo** : Nous avons normalisé la longueur des séquences vidéo en utilisant `pad_sequence` pour que toutes aient la même taille, ce qui est une étape essentielle pour le traitement par LSTM.\n",
    "- **Aplatissage des Caractéristiques** : Les caractéristiques vidéo extraites ont été remodelées pour s'adapter à l'entrée 3D attendue par le LSTM (batch, seq_len, features).\n",
    "\n",
    "#### Modèle LSTM\n",
    "- **Conception** : Le modèle `LSTMClassifier` est une architecture LSTM, avec la possibilité d'ajuster le nombre de couches via `num_layers`. Cela permet de moduler la complexité du modèle en fonction de la nature des données.\n",
    "- **Optimisation** : Le modèle utilise une fonction de perte BCE et un optimiseur Adam, des choix courants pour les tâches de classification binaire.\n",
    "\n",
    "\n",
    "\n",
    "Par manque de temps, nous avons décider de nous concentrer sur l'audio et le texte car l'entraînement et l'extraction de features avec des vidéos peut s'avérer très long et nos ordinateurs peu adaptés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a22b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Padding de X_video pour que toutes les séquences aient la même longueur\n",
    "padded_X_video = pad_sequence([torch.tensor(sequence) for sequence in X_video], batch_first=True, padding_value=0)\n",
    "\n",
    "# Aplatir les deux dernières dimensions pour obtenir une entrée 3D (batch, seq_len, features)\n",
    "padded_X_video = padded_X_video.view(padded_X_video.shape[0], padded_X_video.shape[1], -1)\n",
    "\n",
    "# Conversion de df['turn_after'] en tensor\n",
    "y_tensor = torch.tensor(df['turn_after'].values)\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X_video, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Conversion en DataLoader pour itérer pendant l'entraînement\n",
    "train_data = DataLoader(list(zip(X_train, y_train)), batch_size=2, shuffle=True)\n",
    "test_data = DataLoader(list(zip(X_test, y_test)), batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, num_layers=2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Modifier ici pour ajouter des couches supplémentaires\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        lstm_out, _ = self.lstm(batch)\n",
    "        label_space = self.hidden2label(lstm_out[:,-1,:])  # Utiliser la dernière sortie du LSTM\n",
    "        label_scores = torch.sigmoid(label_space)  # Utiliser sigmoid pour la classification binaire\n",
    "        return label_scores\n",
    "\n",
    "# Paramètres du modèle\n",
    "embedding_dim = padded_X_video.shape[2]  # Basé sur la forme des caractéristiques après l'aplatissement\n",
    "hidden_dim = 64  # Dimension cachée arbitraire, peut être ajustée\n",
    "label_size = 1  # Taille de la sortie, 1 pour la classification binaire\n",
    "\n",
    "# Instanciation du modèle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_layers = 5  # Nombre de couches LSTM\n",
    "model = LSTMClassifier(embedding_dim, hidden_dim, label_size, num_layers=num_layers)\n",
    "model = model.to(device)\n",
    "# Fonction de coût et optimiseur\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25901107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(10):  # Nombre d'époques arbitraire, peut être ajusté\n",
    "    total_loss = 0\n",
    "    for X, y in train_data:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        predictions = model(X)\n",
    "        loss = loss_function(predictions, y.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "# Passage en mode évaluation\n",
    "model.eval()\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_data:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        output = model(X)\n",
    "        test_predictions += output.squeeze().tolist()\n",
    "        test_labels += y.tolist()\n",
    "\n",
    "# Seuil de classification à 0.5 (peut être ajusté)\n",
    "test_predictions = [1 if prob > 0.5 else 0 for prob in test_predictions]\n",
    "\n",
    "# Rapport de classification\n",
    "print(classification_report(test_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63d8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd47a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed880e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a7988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4aca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08ad16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef215d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d507537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956b1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b50a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
